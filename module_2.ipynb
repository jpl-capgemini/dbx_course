{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3cc9805-c97e-4438-8eea-aa6ab2f92a40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Advanced Data Engineering Tutorial Series\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to Module 2! In this notebook, you'll tackle business questions for a hypothetical retail store using synthetic datasets. Through hands-on tasks, you'll clean and enrich data to uncover insights about customer behavior, product sales, and store performance. Each exercise is designed to simulate common challenges faced by data engineers, preparing you to solve practical problems in analytics and reporting.\n",
    "\n",
    "You'll also work through the medallion architecture a layered approach often used in data engineering to organize data pipelines into bronze (raw), silver (cleaned/enriched), and gold (analytics-ready) tables. This helps ensure data quality, scalability, and efficient analytics.\n",
    "\n",
    "## What You'll Do in This Notebook\n",
    "\n",
    "- Load and inspect raw synthetic datasets with intentional data issues\n",
    "- Identify and resolve common data quality problems: nulls, duplicates, and inconsistencies\n",
    "- Add computed columns for deeper analysis (e.g., `total_price`)\n",
    "- Summarize and prepare cleaned data for downstream analytics\n",
    "- Work through the medallion architecture, transforming data from bronze (raw) to gold (dashboard-ready)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Clean and validate complex datasets\n",
    "- Perform multi-step transformations\n",
    "- Apply joins, window functions, and aggregations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c2cd8d5-9381-451b-afe2-e4ad213e725b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 1. Synthetic Datasets Generated with Faker\n",
    "\n",
    "The following synthetic datasets are created using the `Faker` Python package to simulate business scenarios for data cleaning and validation. Each dataset contains intentional data issues (nulls, duplicates, inconsistencies) for hands-on practice.\n",
    "\n",
    "---\n",
    "\n",
    "### **Dataset 1: Transactions**\n",
    "\n",
    "Simulates customer purchase transactions, including details about the transaction, customer, and purchase location. Contains intentional nulls and duplicate records.\n",
    "\n",
    "**Columns:**\n",
    "- `transaction_id`\n",
    "- `customer_id`\n",
    "- `transaction_date`\n",
    "- `location`\n",
    "\n",
    "---\n",
    "\n",
    "### **Dataset 2: Transaction Products**\n",
    "\n",
    "Represents the products associated with each transaction, allowing for multi-product purchases per transaction. Includes nulls and duplicates for cleaning exercises.\n",
    "\n",
    "**Columns:**\n",
    "- `transaction_id`\n",
    "- `product_id`\n",
    "- `quantity`\n",
    "\n",
    "---\n",
    "\n",
    "### **Dataset 3: Product Catalog Reference**\n",
    "\n",
    "A clean product catalog for enriching with product category and price information.\n",
    "\n",
    "**Columns:**\n",
    "- `product_id`\n",
    "- `product_name`\n",
    "- `category`\n",
    "- `standard_price`\n",
    "\n",
    "---\n",
    "\n",
    "### **Dataset 4: Customer Demographics**\n",
    "\n",
    "Contains customer demographic information for advanced enrichment. Includes intentional null values and duplicate entries.\n",
    "\n",
    "**Columns:**\n",
    "- `customer_id`\n",
    "- `customer_name`\n",
    "- `email`\n",
    "- `date_of_birth`\n",
    "- `gender`\n",
    "- `signup_date`\n",
    "- `country`\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "- All datasets are generated with Faker and pandas.\n",
    "- Data issues are intentionally introduced for cleaning and validation practice.\n",
    "- The Transactions and Transaction Products datasets together model multi-product purchases and customer activity.\n",
    "- Use these datasets for exercises in null handling, deduplication, and enrichment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60b2a895-beac-412c-903c-0b1758d2bf6f",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762156619369}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      },
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762166089783}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      },
      "2": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762167369854}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 2
      },
      "3": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"gender\":108},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762175936048}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 3
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install faker pandas\n",
    "\n",
    "from faker import Faker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "fake = Faker()\n",
    "Faker.seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# --- Supermarket Product Names and Categories ---\n",
    "supermarket_products = {\n",
    "    \"Produce\": [\n",
    "        \"Bananas\", \"Apples\", \"Carrots\", \"Lettuce\"\n",
    "    ],\n",
    "    \"Dairy\": [\n",
    "        \"Milk\", \"Cheddar Cheese\", \"Yogurt\", \"Butter\"\n",
    "    ],\n",
    "    \"Bakery\": [\n",
    "        \"Whole Wheat Bread\", \"Bagels\", \"Croissants\", \"Muffins\"\n",
    "    ],\n",
    "    \"Meat & Seafood\": [\n",
    "        \"Chicken Breast\", \"Salmon Fillet\", \"Ground Beef\", \"Pork Chops\"\n",
    "    ],\n",
    "    \"Pantry\": [\n",
    "        \"Pasta\", \"Rice\", \"Canned Beans\", \"Olive Oil\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "product_ids = [f\"P{str(i).zfill(4)}\" for i in range(1, 21)]\n",
    "product_names = []\n",
    "categories = []\n",
    "for cat, names in supermarket_products.items():\n",
    "    product_names.extend(names)\n",
    "    categories.extend([cat] * len(names))\n",
    "\n",
    "locations = [fake.city() for _ in range(5)]\n",
    "# Skew location probabilities (e.g., first location is much more common)\n",
    "location_probs = [0.4, 0.2, 0.15, 0.15, 0.1]\n",
    "\n",
    "# --- Dataset 1: Customer Transactions (normalized) ---\n",
    "n_transactions = 10000\n",
    "\n",
    "# Track locations per customer\n",
    "customer_locations = {}\n",
    "\n",
    "transactions = []\n",
    "transaction_products = []\n",
    "for i in range(n_transactions):\n",
    "    # Introduce missing customer_id for some transactions\n",
    "    customer_id = f\"C{random.randint(1, 2000):04d}\" if random.random() > 0.01 else None\n",
    "    transaction_id = f\"T{10000 + i}\"\n",
    "    transaction_date = fake.date_between(start_date='-900d', end_date='today')\n",
    "    # Assign location, ensuring max 4 locations per customer\n",
    "    if customer_id is not None:\n",
    "        if customer_id not in customer_locations:\n",
    "            customer_locations[customer_id] = set()\n",
    "        available_locations = list(set(locations) - customer_locations[customer_id])\n",
    "        if not available_locations:\n",
    "            location = random.choice(list(customer_locations[customer_id]))\n",
    "        else:\n",
    "            # Skewed location selection for new locations\n",
    "            probs = [location_probs[locations.index(loc)] for loc in available_locations]\n",
    "            probs = np.array(probs) / np.sum(probs)\n",
    "            location = np.random.choice(available_locations, p=probs)\n",
    "            customer_locations[customer_id].add(location)\n",
    "    else:\n",
    "        # Skewed location selection for missing customer_id\n",
    "        location = np.random.choice(locations, p=location_probs)\n",
    "    n_products = np.random.choice([1, 2, 3, 4], p=[0.5, 0.3, 0.15, 0.05])\n",
    "    product_indices = random.sample(range(len(product_ids)), n_products)\n",
    "    transactions.append({\n",
    "        \"transaction_id\": transaction_id,\n",
    "        \"customer_id\": customer_id,\n",
    "        \"transaction_date\": transaction_date,\n",
    "        \"location\": location\n",
    "    })\n",
    "    for product_idx in product_indices:\n",
    "        # Introduce missing product_id and wrong product_id\n",
    "        if random.random() < 0.01:\n",
    "            pid = None\n",
    "        elif random.random() < 0.01:\n",
    "            pid = \"P9999\"  # Non-existent product_id\n",
    "        else:\n",
    "            pid = product_ids[product_idx]\n",
    "        # Introduce missing quantity and negative quantity\n",
    "        if random.random() < 0.01:\n",
    "            qty = None\n",
    "        elif random.random() < 0.01:\n",
    "            qty = -1 * np.random.choice([1, 2, 3, 4, 5])\n",
    "        else:\n",
    "            qty = np.random.choice([1, 2, 3, 4, 5, None], p=[0.3, 0.25, 0.2, 0.15, 0.08, 0.02])\n",
    "        transaction_products.append({\n",
    "            \"transaction_id\": transaction_id,\n",
    "            \"product_id\": pid,\n",
    "            \"quantity\": qty\n",
    "        })\n",
    "\n",
    "df_transactions = pd.DataFrame(transactions)\n",
    "df_transaction_products = pd.DataFrame(transaction_products)\n",
    "\n",
    "# --- Dataset 2: Product Catalog Reference (normalized) ---\n",
    "catalog = []\n",
    "for pid, pname, cat in zip(product_ids, product_names, categories):\n",
    "    entry = {\n",
    "        \"product_id\": pid,\n",
    "        \"product_name\": pname,\n",
    "        \"category\": cat,\n",
    "        \"standard_price\": round(np.random.uniform(5, 15), 2)\n",
    "    }\n",
    "    catalog.append(entry)\n",
    "df_catalog = pd.DataFrame(catalog)\n",
    "\n",
    "# --- Dataset 3: Customer Demographics (normalized) ---\n",
    "n_customers = 2000\n",
    "customer_ids = [f\"C{str(i).zfill(4)}\" for i in range(1, n_customers + 1)]\n",
    "demographics = []\n",
    "for cid in customer_ids:\n",
    "    dob = fake.date_of_birth(minimum_age=18, maximum_age=80)\n",
    "    # Introduce missing name and wrong gender\n",
    "    name = fake.name() if random.random() > 0.01 else None\n",
    "    gender = np.random.choice([\"Male\", \"Female\", \"Other\", None, \"Unknown\"], p=[0.47, 0.47, 0.03, 0.01, 0.02])\n",
    "    # Construct email as \"lastname.firstname@example.com\" if name is present\n",
    "    if name is not None:\n",
    "        parts = name.split()\n",
    "        if len(parts) >= 2:\n",
    "            firstname = parts[0].replace(\".\", \"\").lower()\n",
    "            lastname = parts[-1].replace(\".\", \"\").lower()\n",
    "            email = f\"{lastname}.{firstname}@example.com\"\n",
    "        else:\n",
    "            email = \"invalid_email\"\n",
    "    else:\n",
    "        # 50% chance of valid email using a random name\n",
    "        if random.random() < 0.5:\n",
    "            rand_name = fake.name()\n",
    "            parts = rand_name.split()\n",
    "            if len(parts) >= 2:\n",
    "                firstname = parts[0].replace(\".\", \"\").lower()\n",
    "                lastname = parts[-1].replace(\".\", \"\").lower()\n",
    "                email = f\"{lastname}.{firstname}@example.com\"\n",
    "            else:\n",
    "                email = \"invalid_email\"\n",
    "        else:\n",
    "            email = \"invalid_email\"\n",
    "    demographics.append({\n",
    "        \"customer_id\": cid,\n",
    "        \"customer_name\": name,\n",
    "        \"email\": email,\n",
    "        \"date_of_birth\": dob,\n",
    "        \"gender\": gender,\n",
    "        \"signup_date\": fake.date_between(start_date=datetime.today() - timedelta(days=5*365), end_date='today'),\n",
    "        \"country\": fake.country()\n",
    "    })\n",
    "df_demographics = pd.DataFrame(demographics)\n",
    "\n",
    "# --- Introduce duplicate and invalid records ---\n",
    "n_demo_dupes = random.randint(10, 30)\n",
    "df_demographics = pd.concat([df_demographics, df_demographics.sample(n_demo_dupes, random_state=3)], ignore_index=True)\n",
    "df_demographics.loc[df_demographics.sample(frac=0.01, random_state=4).index, \"email\"] = \"invalid_email\"\n",
    "\n",
    "n_trans_dupes = random.randint(10, 30)\n",
    "df_transactions = pd.concat([df_transactions, df_transactions.sample(n_trans_dupes, random_state=5)], ignore_index=True)\n",
    "\n",
    "n_prod_dupes = random.randint(10, 30)\n",
    "df_transaction_products = pd.concat([df_transaction_products, df_transaction_products.sample(n_prod_dupes, random_state=6)], ignore_index=True)\n",
    "\n",
    "# --- Convert to Spark DataFrames ---\n",
    "df_transactions_spark = spark.createDataFrame(df_transactions)\n",
    "df_transaction_products_spark = spark.createDataFrame(df_transaction_products)\n",
    "df_catalog_spark = spark.createDataFrame(df_catalog)\n",
    "df_demographics_spark = spark.createDataFrame(df_demographics)\n",
    "\n",
    "display(df_transactions_spark)\n",
    "display(df_transaction_products_spark)\n",
    "display(df_catalog_spark)\n",
    "display(df_demographics_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "865ef1a2-7779-49c8-a903-2501c2f792ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_transactions_spark.write.mode(\"overwrite\").saveAsTable(\"dbx_course_catalog.landing.transactions\")\n",
    "df_transaction_products_spark.write.mode(\"overwrite\").saveAsTable(\"dbx_course_catalog.landing.transaction_products\")\n",
    "df_catalog_spark.write.mode(\"overwrite\").saveAsTable(\"dbx_course_catalog.landing.catalog\")\n",
    "df_demographics_spark.write.mode(\"overwrite\").saveAsTable(\"dbx_course_catalog.landing.demographics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c4fbad4-f540-4225-a2d6-f459e1c5e08b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 0: Medallion Architecture: Theory and Application in This Notebook\n",
    "\n",
    "The **medallion architecture** is a layered approach to organizing data pipelines. It divides data into three main layers:\n",
    "\n",
    "- **Bronze (Raw):** The initial landing zone for ingested data. This layer contains raw, unvalidated records directly from source systems, often with schema inconsistencies, duplicates, and missing values.\n",
    "- **Silver (Validated):** The cleaned and enriched layer. Here, data is deduplicated, validated, and normalized. Business rules are applied to correct errors and inconsistencies, making the data reliable for downstream analytics.\n",
    "- **Gold (Enriched/Presentation):** The analytics-ready layer. Data is aggregated, modeled, and transformed to support reporting, dashboards, and advanced analytics.\n",
    "\n",
    "> The medallion architecture isn't a set of strict rules. It's more of a flexible set of guidelines. You can adapt the boundaries and requirements of each layer to fit your business needs, data sources, and analytics goals. The key is to maintain a clear separation between raw, cleaned, and analytics-ready data, but the specific transformations and validations can vary depending on your use case.\n",
    "\n",
    "**How we move from raw to silver to gold in this notebook:**\n",
    "\n",
    "1. **Bronze Layer:**  \n",
    "   - We start by generating synthetic datasets with intentional data issues (nulls, duplicates, inconsistencies) and save them to the `dbx_course_catalog.landing` schema.  \n",
    "   - These raw tables represent the bronze layer.\n",
    "\n",
    "2. **Silver Layer:**  \n",
    "   - We load the raw tables and perform data cleaning: removing duplicates, handling nulls, correcting invalid values, and enriching records.  \n",
    "   - Cleaned tables are saved to the `dbx_course_catalog.silver` schema, representing the silver layer.\n",
    "\n",
    "3. **Gold Layer:**  \n",
    "   - We further transform the silver tables by joining, aggregating, and deriving new features (e.g., total sales, customer segments, product rankings).  \n",
    "   - These analytics-ready tables are saved to the `dbx_course_catalog.gold` schema, forming the gold layer for reporting and advanced analysis.\n",
    "\n",
    "This stepwise approach ensures that only high-quality, business-ready data is used for insights and decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20458f45-fa75-490a-8e33-c0dbe4710340",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1: Data Cleaning & Validation\n",
    "\n",
    "Before we dive into answering business questions for our hypothetical store, our first mission is to clean the data. Unfortunately data is rarely perfect, every dataset arrives with its own quirks—missing values, duplicates, and odd inconsistencies. Regardless of the analysis ahead, we must ensure our data is trustworthy and ready for use.\n",
    "\n",
    "Cleaning data isn't just a technical task; it often involves business decisions, like whether to fix or remove incomplete records. In this notebook, you won't need to make those choices yourself—the exercises will guide you step by step, explaining what to do in each scenario.\n",
    "\n",
    "**Datasets:**\n",
    "1. Transactions\n",
    "2. Transaction Products\n",
    "3. Product Catalog\n",
    "4. Customer Demographics\n",
    "\n",
    "**Key Activities (for each dataset):**\n",
    "- Load and inspect the dataset\n",
    "- Detect and handle null values in critical columns\n",
    "- Remove duplicate records\n",
    "- Identify and correct inconsistent values (e.g., prices, emails, dates)\n",
    "- Summarize cleaned data\n",
    "\n",
    "Exercises (apply to each dataset):**\n",
    "1. For each dataset, answer the following:\n",
    "   - Which rows in the Transactions dataset have missing `customer_id`?  \n",
    "   *Delete records in the Transactions dataset where `customer_id` is null. Use the `filter` function to delete records without `customer_id`.*\n",
    "   - Which rows in the Transaction Products dataset have missing `quantity` or `product_id`?  \n",
    "     *Delete records with missing `quantity` or `product_id`. Also, to correct negative `quantity` values, use the `when`/`otherwise` function: set `quantity = when(quantity < 0, -quantity).otherwise(quantity)`.*\n",
    "   - Which rows in the Demographics dataset have missing `customer_name`, missing `gender`, or an invalid email?  \n",
    "     *If `gender` is missing, assign \"Unknown\" (tip: use the `lit` function to set a constant value).*\n",
    "        > **Extra (Advanced) Exercise:**  \n",
    "        For records with missing `email` but available `customer_name`, set `email` as `lastname.firstname@example.com`. If both are missing, delete the record. Create a new column (e.g., `is_manually_repaired`) to flag records that were manually repaired. \n",
    "        \n",
    "        > If `email` is available but `customer_name` is missing, attempt to extract the name from the email address (e.g., split on \".\" and \"@\" and use as first/last name). This logic can be challenging. If you can't get it to work, delete records where `customer_name` is missing but `email` is present and continue with the exercises.\n",
    "\n",
    "2. Remove duplicate records based on unique keys.  \n",
    "   *Tip: Use the `dropDuplicates()` function.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bccce94-50d2-4720-af3a-b0b52056fbcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_transactions_spark = spark.table(\"dbx_course_catalog.landing.transactions\")\n",
    "df_transaction_products_spark = spark.table(\"dbx_course_catalog.landing.transaction_products\")\n",
    "df_catalog_spark = spark.table(\"dbx_course_catalog.landing.catalog\")\n",
    "df_demographics_spark = spark.table(\"dbx_course_catalog.landing.demographics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1b6db03-d914-4dda-8ca8-f8b8a9e0043e",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762247854119}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      },
      "2": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"customer_id\":137},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762258782199}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 2
      },
      "3": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"country\":210},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762176183294}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 3
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, count, lit\n",
    "\n",
    "# 1. Delete all rows with missing critical IDs\n",
    "\n",
    "# 2. Correct negative quantity values in df_transaction_products_spark by multiplying with -1\n",
    "\n",
    "# 3. Clean gender, email and customer_name fields, flag manually repaired records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8eb1c7a1-a9f4-4129-9a89-371fe399ddad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. Remove duplicates from tables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a68f492b-1162-4fb4-ba87-9d7058e4310e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Saving and Loading tables\n",
    "Before saving, create the schema if it does not exist using `spark.sql`. Then save your cleaned tables to the `dbx_course_catalog.silver` schema with `.write.saveAsTable(\"dbx_course_catalog.silver.<table_name>\")`. After saving, load the tables for further analysis, use `spark.table(\"dbx_course_catalog.silver.<table_name>\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b565aa5-a1a9-4b55-8b9a-2aec440ec7cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save tables in silver schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a156dcad-574e-4470-b57d-99f3cbf0cb87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load tables from silver schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fa68249-fca1-4e3c-a26e-fabd118bb1a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Advanced Data Engineering Exercises\n",
    "\n",
    "Below are categorized exercises that leverage derived columns and advanced data engineering techniques:\n",
    "\n",
    "#### 1. **Data Enrichment & Feature Engineering**\n",
    "- Create `total_price` in the Transactions dataset (`quantity * unit_price`)\n",
    "> **Tip:** To calculate `total_price`, join the Transaction Products with the Product Catalog on `product_id`. Join the resulting table to Transactions on `transaction_id`. \n",
    "- Add `customer_age` and `is_adult` in Customer Demographics\n",
    "> **Tip:** To calculate `customer_age`, use the `datediff` and `current_date` functions with `date_of_birth`.\n",
    "- Flag `is_high_value_transaction` in Transactions\n",
    "\n",
    "#### 2. **Temporal & Hierarchical Analysis**\n",
    "- Extract `transaction_week` and `transaction_quarter` from transaction dates\n",
    "> **Tip:** Need to extract the week or quarter from a date in PySpark? Search online for built-in PySpark functions that make calculating week and quarter straightforward.\n",
    "- Calculate `days_since_last_purchase` for each customer\n",
    "> **Tip:** To calculate metrics like `days_since_last_purchase`, use window functions to partition your data by `customer_id` and order by `transaction_date`. The `lag` function lets you access the previous transaction date for each customer, enabling you to compute the difference between current and prior purchases.\n",
    "\n",
    "> Window functions in Spark allow you to perform calculations across a set of rows that are related to the current row, without collapsing them into a single output row. This is useful for tasks like calculating running totals, ranking, or accessing previous/next values in a partitioned group of data. You define a \"window\" using `PARTITION BY` (to group rows) and `ORDER BY` (to sort within each group), and then apply functions such as `lag`, `lead`, `row_number`, `rank`, or aggregate functions like `sum` or `avg` over that window. This enables advanced analytics like time-based calculations, cohort analysis, and more, all while keeping the original row structure. \n",
    "- Aggregate sales by product, and location\n",
    "\n",
    "At this point, you've been introduced to the key functions you'll need for these exercises. As you work through the next steps, feel free to use these functions or consult the PySpark and Databricks documentation for any additional tools or techniques you may need.\n",
    "#### 3. **Ranking**\n",
    "- Rank products by sales volume (`product_popularity`)\n",
    "- Flag top selling products with `is_top_seller`\n",
    "- Assign customer segments based on purchase frequency (e.g. platinum, gold, silver, bronze)\n",
    "\n",
    "#### 4. **Segmentation & Cohort Analysis**\n",
    "- Categorize customers by `age_group` and `tenure_days`\n",
    "- Estimate `customer_lifetime_value` (total spend per customer) by joining transactions and demographics\n",
    "\n",
    "\n",
    "These exercises will help you practice multi-step transformations, complex aggregations, and robust data validation, preparing your datasets for deeper analytics and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d35bc7eb-f8b0-452b-8652-4eaeb39aa9f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 1. **Data Enrichment & Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a35a63bb-5ad1-4c1a-9691-991224078acd",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762258168235}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, lit\n",
    "\n",
    "# 1. Data Enrichment & Feature Engineering\n",
    "\n",
    "# Join transaction products with catalog and transactions to calculate total price and flag high value transactions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04005a47-b709-4b2e-92a3-df8a938bad6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "When performing left joins between DataFrames (e.g., joining transactions with products or customer demographics), it's possible to produce rows where for example the `transaction_id` or `customer_id` column is null. This typically means that the join key from the left DataFrame did not find a matching record in the right DataFrame, indicating missing or unmatched data. Such nulls can signal data integrity issues, such as orphaned records or incomplete relationships. For initial cleaning, we will just delete records with null `transaction_id` and `customer_id` values to ensure only valid, fully matched transactions are retained for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a156af1a-dc36-4a76-bb25-3a48e9f2b027",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_enriched_txn = df_enriched_txn.filter(col(\"customer_id\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2687176-3267-4b1c-9c76-1581fa89f480",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762253446207}",
       "filterBlob": "{\"version\":1,\"filterGroups\":[],\"syncTimestamp\":1762253464837}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate customer age and flag adults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20443cb3-13fa-4313-9d0b-db8fb7bf4970",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2. **Temporal & Hierarchical Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b636946f-1df4-45a3-bf12-7932f8c1f6dc",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762270517275}",
       "filterBlob": "{\"version\":1,\"filterGroups\":[],\"syncTimestamp\":1762257365162}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      },
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762257346916}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import weekofyear, quarter, lag as spark_lag, sum as spark_sum\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# 2. Temporal & Hierarchical Analysis\n",
    "\n",
    "# Extract transaction week and quarter\n",
    "\n",
    "# Days since last purchase per customer\n",
    "\n",
    "# Aggregate sales by product, location\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b001bd45-0358-4df3-8f94-9f224ba331b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 3. **Ranking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61ce692c-f6d1-4dd9-8436-7aef9297a4eb",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762247687647}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count as spark_count, rank\n",
    "\n",
    "# 3. Ranking\n",
    "\n",
    "# Product popularity ranking\n",
    "\n",
    "# Customer segments by purchase frequency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c714b87-bbbf-4b64-b928-9745bb6a514b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 4. **Segmentation & Cohort Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "211b138c-1d06-4bf5-a375-27235dfba45b",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"customer_id\":137},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762257998770}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. Segmentation\n",
    "\n",
    "# Age group and tenure\n",
    "\n",
    "# Estimate customer lifetime value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bdec3061-cd38-4f18-aa43-7cc4eccb4101",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Saving gold tables\n",
    "Save your final tables in a new schema, `dbx_course_catalog.gold`.  \n",
    "Make sure to create the schema if it doesn't already exist before saving the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b25635a5-dbdc-4c25-bd3c-37046074985a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# save tables to gold schema"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "module_2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
