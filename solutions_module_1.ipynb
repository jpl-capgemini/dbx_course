{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3182049-787b-4bb2-8242-1bf6055cdbd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Welcome to Databricks & PySpark: Module 1\n",
    "\n",
    "---\n",
    "\n",
    "## Course Introduction\n",
    "\n",
    "Welcome to the first module of our Databricks course! In this notebook, you'll get hands-on experience with Databricks and PySpark, learning how to process and analyze data at scale.\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "By the end of this module, you will be able to:\n",
    "\n",
    "1. **Set up your Databricks environment**  \n",
    "   Mount data sources and load CSV files.\n",
    "2. **Understand PySpark basics**  \n",
    "   Create Spark DataFrames and perform basic data operations.\n",
    "3. **Explore your data**  \n",
    "   Generate summary statistics, aggregate data, and perform exploratory analysis.\n",
    "4. **Write data efficiently**  \n",
    "   Save your results in Parquet and Delta formats, and understand the benefits of Delta Lake.\n",
    "5. **Apply your knowledge**  \n",
    "   Complete practical exercises, including filtering transactions, grouping and counting products, and saving filtered data as Delta tables.\n",
    "\n",
    "---\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63bdb768-22f3-4b43-b55f-90b23b3c8aa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Objective 1: Setting Up Your Databricks Environment\n",
    "\n",
    "Follow these steps to set up your Databricks environment, create a catalog and schema, and load your first CSV file as a table.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Create a Catalog and Schema using the Databricks SQL Editor\n",
    "\n",
    "1. Open the **Databricks SQL Editor** from the workspace sidebar, or use a notebook cell with `%sql` at the top to run SQL code directly in your notebook.\n",
    "2. Run the following SQL commands to create a new catalog and a landing schema:\n",
    "\n",
    "   ``` sql\n",
    "   CREATE CATALOG IF NOT EXISTS dbx_course_catalog;\n",
    "   USE CATALOG dbx_course_catalog;\n",
    "\n",
    "   CREATE SCHEMA IF NOT EXISTS landing;\n",
    "   USE SCHEMA landing;`\n",
    "   ```\n",
    "   \n",
    "\n",
    "   Replace `dbx_course_catalog` and `landing` with your preferred names if desired.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f45dcd5a-5324-40bc-8d30-c022da84af8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE CATALOG IF NOT EXISTS dbx_course_catalog;\n",
    "USE CATALOG dbx_course_catalog;\n",
    "\n",
    "CREATE SCHEMA IF NOT EXISTS landing;\n",
    "USE SCHEMA landing;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10d8d37a-ec0e-4940-bc0f-aa2ac738c1de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Upload and Load a CSV File as a Table\n",
    "\n",
    "1. In the sidebar, click **Catalog** and navigate to your catalog and schema.\n",
    "2. Click **+ New > Add data**.\n",
    "3. Upload your CSV file to a Unity Catalog volume or select an existing file.\n",
    "4. In the **Add data** UI:\n",
    "   - Select your catalog (`dbx_course_catalog`) and schema (`landing`).\n",
    "   - (Optional) Edit the table name.\n",
    "   - Click **Create table** to load the CSV as a managed table.\n",
    "\n",
    "\n",
    "Alternatively, you can use SQL to create a table from a CSV file:\n",
    "\n",
    "```sql\n",
    "CREATE TABLE landing.customer_transactions\n",
    "USING CSV\n",
    "OPTIONS (\n",
    "  path = 'yourpath/customer_transactions.csv',\n",
    "  header = 'true',\n",
    "  inferSchema = 'true'\n",
    ");\n",
    "```\n",
    "Note: This approach can cause issues in the Databricks Community (Free) Edition due to limitations with DBFS (Databricks File System), which is a distributed file system used by Databricks to interact with cloud storage. In the free edition, access to DBFS and Unity Catalog volumes is restricted, so this method is not used in this class.\n",
    "\n",
    "---\n",
    "\n",
    "You have now set up your environment, created a catalog and schema, and loaded your first table!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80fcc2f5-33c3-43e7-86a3-a6d6bda3d026",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Objective 2: PySpark Basics\n",
    "\n",
    "In this section, you'll learn how to use PySpark to work with data in Databricks.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Create a Spark DataFrame from a Table\n",
    "\n",
    "Let's start by creating a Spark DataFrame from the `customer_transactions` table you loaded earlier using `spark.table()`, and then display the first few rows with `.show()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24275eb2-24c0-490f-a26c-d6a1aa07c16a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Solution"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(\"dbx_course_catalog.landing.customer_transactions\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "829b13f3-b3e5-4f3e-954c-08c10661bb5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "### 2. PySpark Exercises\n",
    "\n",
    "Let's explore the data we've just loaded and get a feel for how PySpark works in Databricks. In this section, we'll walk through some basic operations you can perform on your DataFrame. These exercises will help you understand how to select specific columns, filter rows, sort data, group and aggregate, remove duplicates, rename columns, and save your results as new tables.\n",
    "\n",
    "**Note:** The columns in your dataset are: `transaction_id`, `customer_id`, `date`, `product`, `quantity`, `price_per_unit`, `location`, and `data_issue`.  \n",
    "- Each row represents a single product within a transaction. Multiple rows can share the same `transaction_id` if they are part of the same transaction.\n",
    "\n",
    "In the following exercises, you'll practice essential PySpark DataFrame operations using the `customer_transactions` data. Each exercise specifies the columns or conditions you'll use:\n",
    "\n",
    "1. **Select specific columns:**  \n",
    "   Extract only the `customer_id` and `price_per_unit` columns from the DataFrame.\n",
    "\n",
    "2. **Filter rows based on a condition:**  \n",
    "   Retrieve all transactions where `price_per_unit` is greater than 100 euros.\n",
    "\n",
    "3. **Order and sort data:**  \n",
    "   Sort the DataFrame by `price_per_unit` in descending order to see the largest transactions first.\n",
    "\n",
    "4. **Group by and aggregate:**  \n",
    "   Group the data by `customer_id` and calculate the total amount spent (sum of `quantity * price_per_unit` for all products per customer).\n",
    "\n",
    "5. **Drop duplicates:**  \n",
    "   Remove duplicate records.\n",
    "\n",
    "6. **Rename columns:**  \n",
    "   Rename the `price_per_unit` column to `unit_price` for clarity.\n",
    "\n",
    "7. **Save filtered data as a new table:**  \n",
    "   Save the filtered DataFrame (transactions with `price_per_unit` > 100 euros) as a new table called `high_value_transactions` in your schema.\n",
    "\n",
    "8. **Append data to an existing table:**  \n",
    "   Add this high-value transactions to the `high_value_transactions` table:\n",
    "\n",
    "| transaction_id                           | customer_id | date       | product | quantity | price_per_unit | location  | data_issue |\n",
    "|------------------------------------------|-------------|------------|---------|----------|---------------|-----------|------------|\n",
    "| 23b1ca18-f88a-46ce-b22c-e7fa4673050f     | 4828        | 2025-03-06 | Monitor | 3        | 300           | Amsterdam | null       |\n",
    "> **Tip for Exercise 8:**  \n",
    "> To practice appending data, first create a new DataFrame containing just this single row.  \n",
    "> - Use `import datetime` and `from pyspark.sql import Row` to specify the date and define the row.\n",
    "> - Manually define the schema using `StructType` and `StructField`.  \n",
    "> - Save this DataFrame as a new table, then use `.write.mode(\"append\")` to add it to your existing `high_value_transactions` table.\n",
    "\n",
    "These exercises will help you become comfortable with selecting, filtering, sorting, aggregating, deduplicating, renaming, and saving data using PySpark in Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10cc4461-e216-4e79-81cf-b53b56c64ae2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#### a. Select specific columns\n",
    "\n",
    "df_selected_cols = df.select(\"customer_id\", \"price_per_unit\")\n",
    "display(df_selected_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00a9d466-9ef5-4671-852b-f6b6c952a749",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#### b. Filter rows\n",
    "\n",
    "df_filtered_price = df.filter(df.price_per_unit > 100)\n",
    "display(df_filtered_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "904a2b10-2db2-4be2-97e8-15df6c2ed906",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#### c. Order and sort data\n",
    "\n",
    "df_sorted_price = df.orderBy(df.price_per_unit.desc())\n",
    "display(df_sorted_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2c5c5cf-dc11-4ddf-a47a-0e9be20b89bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#### d. Group by and aggregate\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_grouped_total = df.withColumn(\n",
    "    \"total_amount\", F.col(\"quantity\") * F.col(\"price_per_unit\")\n",
    ").groupBy(\"customer_id\").agg(F.sum(\"total_amount\").alias(\"total_spent\"))\n",
    "display(df_grouped_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1ec5749-f8a7-4280-a4b0-e619e40510d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#### e. Drop duplicates\n",
    "\n",
    "df_deduped_all = df.dropDuplicates()\n",
    "print(f\"Rows removed: {df.count() - df_deduped_all.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4125bc6-b166-419f-b9de-22dbde7bf091",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#### f. Rename columns\n",
    "\n",
    "df_renamed_unit_price = df.withColumnRenamed(\"price_per_unit\", \"unit_price\")\n",
    "display(df_renamed_unit_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a915b3f7-2ea2-4e43-bf91-b12441e4845c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#### g. Create a new table\n",
    "\n",
    "df_filtered_price.write.mode(\"overwrite\").saveAsTable(\"dbx_course_catalog.landing.high_value_transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8efe3412-b29b-42a6-9cda-47380beceaca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#### h. Insert into an existing table\n",
    "\n",
    "from pyspark.sql import Row\n",
    "import datetime\n",
    "\n",
    "new_row = Row(\n",
    "    transaction_id=\"23b1ca18-f88a-46ce-b22c-e7fa4673050f\",\n",
    "    customer_id=4828,\n",
    "    date=datetime.date(2025, 3, 6),\n",
    "    product=\"Monitor\",\n",
    "    quantity=3,\n",
    "    price_per_unit=300,\n",
    "    location=\"Amsterdam\",\n",
    "    data_issue=None\n",
    ")\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, DoubleType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"transaction_id\", StringType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"date\", DateType(), True),\n",
    "    StructField(\"product\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"price_per_unit\", DoubleType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"data_issue\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_new = spark.createDataFrame([new_row.asDict()], schema=schema)\n",
    "df_new.write.mode(\"append\").saveAsTable(\"dbx_course_catalog.landing.high_value_transactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1f9653f-909f-4d65-b533-cfa168423559",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Objective 3: Data Exploration and Analysis\n",
    "\n",
    "In this section, you'll learn how to explore and analyze your data using PySpark. You'll generate summary statistics, perform aggregations, and conduct exploratory analysis to better understand the patterns and trends in your dataset. These skills are essential for uncovering insights and preparing your data for further processing or machine learning tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### Objective 3 Exercises: Data Exploration and Analysis\n",
    "\n",
    "Practice the following exercises to deepen your understanding of data exploration and analysis with PySpark.\n",
    "To import PySpark functions for analysis: `from pyspark.sql import functions as F`\n",
    "\n",
    "1. **Summary Statistics:**  \n",
    "   Use the `.describe()` method to generate summary statistics (count, mean, stddev, min, max) for the `quantity` and `price_per_unit` columns.\n",
    "\n",
    "2. **Value Counts:**  \n",
    "   Count the number of transactions for each unique `product` using `.groupBy().count()`.\n",
    "\n",
    "3. **Missing Data Analysis:**  \n",
    "   Find out how many rows have missing (`null`) values in the `data_issue` column. ( Use the `F.col()` function to select a column and use `.isNull()` to check for null values)\n",
    "\n",
    "4. **Top Customers:**  \n",
    "   Identify the top 5 customers who spent the most in total (sum of `quantity * price_per_unit`), and display their `customer_id` and total spent. You can use `.alias()` to rename aggregate columns for clarity.\n",
    "\n",
    "5. **Monthly Trends:**  \n",
    "   Group transactions by month (extract month from the `date` column using `F.month`) and calculate the total quantity sold each month.\n",
    "\n",
    "6. **Location Analysis:**  \n",
    "   For each `location`, compute the average `price_per_unit` and the total number of transactions.\n",
    "\n",
    "7. **Data Issue Investigation:**  \n",
    "   List all unique values found in the `data_issue` column and count how many times each occurs.\n",
    "\n",
    "> 8. **Optional (difficult) exercise: Outlier Detection:**  \n",
    "   Find all transactions where `quantity` is greater than 10 or `price_per_unit` is more than 3 standard deviations above the mean.\n",
    "\n",
    "Try to solve each exercise using PySpark DataFrame operations. These tasks will help you build skills in summarizing, grouping, filtering, and analyzing your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b2144e0-34f3-4784-94f4-b4ecbb2a255a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1. Summary Statistics for 'quantity' and 'price_per_unit'\n",
    "display(df.select(\"quantity\", \"price_per_unit\").describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4dde6b93-360b-411f-8805-1d016b3c4233",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Value Counts for each unique 'product'\n",
    "df_product_counts = df.groupBy(\"product\").count()\n",
    "display(df_product_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca7c3da2-a365-4517-8d4a-25eee3ec2052",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Missing Data Analysis for 'data_issue'\n",
    "missing_data_issue_count = df.filter(F.col(\"data_issue\").isNotNull()).count()\n",
    "print(f\"Rows with data_issue: {missing_data_issue_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4bd26ee7-71a1-41ec-9ad5-e7d487305fa1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. Top 5 Customers by Total Spent\n",
    "df_top_customers = (\n",
    "    df.withColumn(\"total_amount\", F.col(\"quantity\") * F.col(\"price_per_unit\"))\n",
    "      .groupBy(\"customer_id\")\n",
    "      .agg(F.sum(\"total_amount\").alias(\"total_spent\"))\n",
    "      .orderBy(F.col(\"total_spent\").desc())\n",
    "      .limit(5)\n",
    ")\n",
    "display(df_top_customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54d07d60-e7c2-4198-adff-d2c1669c19f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5. Monthly Trends: Total quantity sold per month\n",
    "df_monthly_trends = (\n",
    "    df.withColumn(\"month\", F.month(\"date\"))\n",
    "      .groupBy(\"month\")\n",
    "      .agg(F.sum(\"quantity\").alias(\"total_quantity\"))\n",
    "      .orderBy(\"month\")\n",
    ")\n",
    "display(df_monthly_trends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ceaadd5-93e5-46f0-8462-d8fa46a3399f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 6. Location Analysis: Avg price_per_unit and total transactions per location\n",
    "df_location_analysis = (\n",
    "    df.groupBy(\"location\")\n",
    "      .agg(\n",
    "          F.avg(\"price_per_unit\").alias(\"avg_price_per_unit\"),\n",
    "          F.count(\"*\").alias(\"transaction_count\")\n",
    "      )\n",
    ")\n",
    "display(df_location_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e79759ee-560e-4903-981f-28c3520a3a02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 7. Data Issue Investigation: Unique values and counts in 'data_issue'\n",
    "df_data_issue_counts = (\n",
    "    df.groupBy(\"data_issue\")\n",
    "      .count()\n",
    "      .orderBy(F.col(\"count\").desc())\n",
    ")\n",
    "display(df_data_issue_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55346d22-dc9c-48a6-ab98-2a5545a14e3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 8. Outlier Detection: quantity > 10 or price_per_unit > mean + 3*stddev\n",
    "stats = df.select(\n",
    "    F.mean(\"price_per_unit\").alias(\"mean\"),\n",
    "    F.stddev(\"price_per_unit\").alias(\"stddev\")\n",
    ").collect()[0]\n",
    "mean_price = stats[\"mean\"]\n",
    "stddev_price = stats[\"stddev\"]\n",
    "threshold = mean_price + 3 * stddev_price\n",
    "\n",
    "df_outliers = df.filter(\n",
    "    (F.col(\"quantity\") > 10) | (F.col(\"price_per_unit\") > threshold)\n",
    ")\n",
    "display(df_outliers)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5588504418158820,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "solutions_module_1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
